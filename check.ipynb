{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95bf6813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc2fdc21",
   "metadata": {},
   "outputs": [
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/SIML/sy/group_theory/Continual_learning/pangea_qwen3_tokenizer'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHFValidationError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/conda_envs/vllm/lib/python3.12/site-packages/transformers/utils/hub.py:479\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    478\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/conda_envs/vllm/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:106\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mrepo_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfrom_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arg_name == \u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/conda_envs/vllm/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:154\u001b[39m, in \u001b[36mvalidate_repo_id\u001b[39m\u001b[34m(repo_id)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m repo_id.count(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    155\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must be in the form \u001b[39m\u001b[33m'\u001b[39m\u001b[33mrepo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnamespace/repo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Use `repo_type` argument if needed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m     )\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX.match(repo_id):\n",
      "\u001b[31mHFValidationError\u001b[39m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/SIML/sy/group_theory/Continual_learning/pangea_qwen3_tokenizer'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mHFValidationError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tok=\u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/data/SIML/sy/group_theory/Continual_learning/pangea_qwen3_tokenizer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/conda_envs/vllm/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:1073\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1070\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n\u001b[32m   1072\u001b[39m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1073\u001b[39m tokenizer_config = \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[32m   1075\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = tokenizer_config[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/conda_envs/vllm/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:905\u001b[39m, in \u001b[36mget_tokenizer_config\u001b[39m\u001b[34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[39m\n\u001b[32m    902\u001b[39m     token = use_auth_token\n\u001b[32m    904\u001b[39m commit_hash = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m905\u001b[39m resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    913\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    914\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    922\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/conda_envs/vllm/lib/python3.12/site-packages/transformers/utils/hub.py:322\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    265\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    266\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    267\u001b[39m     **kwargs,\n\u001b[32m    268\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    269\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    271\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/conda_envs/vllm/lib/python3.12/site-packages/transformers/utils/hub.py:532\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    525\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPermissionError at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m when downloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    526\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCheck cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    527\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m2) a previous download was canceled and the lock file needs manual removal.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    528\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    530\u001b[39m \u001b[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001b[39;00m\n\u001b[32m    531\u001b[39m resolved_files = [\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m     \u001b[43m_get_cache_file_to_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[32m    534\u001b[39m ]\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files):\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved_files\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/conda_envs/vllm/lib/python3.12/site-packages/transformers/utils/hub.py:143\u001b[39m, in \u001b[36m_get_cache_file_to_return\u001b[39m\u001b[34m(path_or_repo_id, full_filename, cache_dir, revision, repo_type)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_cache_file_to_return\u001b[39m(\n\u001b[32m    136\u001b[39m     path_or_repo_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    137\u001b[39m     full_filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    141\u001b[39m ):\n\u001b[32m    142\u001b[39m     \u001b[38;5;66;03m# We try to see if we have a cached version (not up to date):\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     resolved_file = \u001b[43mtry_to_load_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m resolved_file != _CACHED_NO_EXIST:\n\u001b[32m    147\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/conda_envs/vllm/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:106\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mzip\u001b[39m(signature.parameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[32m    103\u001b[39m     kwargs.items(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[32m    104\u001b[39m ):\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mrepo_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfrom_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m arg_name == \u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    109\u001b[39m         has_token = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/conda_envs/vllm/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:154\u001b[39m, in \u001b[36mvalidate_repo_id\u001b[39m\u001b[34m(repo_id)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRepo id must be a string, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(repo_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m repo_id.count(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    155\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must be in the form \u001b[39m\u001b[33m'\u001b[39m\u001b[33mrepo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnamespace/repo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Use `repo_type` argument if needed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m     )\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX.match(repo_id):\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    161\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must use alphanumeric chars or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m--\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m\u001b[33m are\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m forbidden, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m cannot start or end the name, max length is 96:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    163\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m     )\n",
      "\u001b[31mHFValidationError\u001b[39m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/SIML/sy/group_theory/Continual_learning/pangea_qwen3_tokenizer'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "source": [
    "tok=AutoTokenizer.from_pretrained(\"/data/SIML/sy/group_theory/Continual_learning/pangea_qwen3_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487cdfbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151669"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b494169",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tok=AutoTokenizer.from_pretrained(\"/data/SIML/sy/group_theory/Continual_learning/pangea_qwen3_tokenizer_safe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ad6b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157433"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e7fa0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_TOK_DIR = \"./qwen3_tokenizer\"         # 확장 전 토크나이저\n",
    "EXP_TOK_DIR  = \"./expanded_qwen3_tokenizer_safe\"    # 확장 후 토크나이저(OUT_DIR)\n",
    "\n",
    "# \"서브워드 평균 초기화\"까지 끝낸 뒤 저장한 디렉토리\n",
    "# (init_new_token_embeddings_by_subword_mean.py에서 --save_dir로 준 경로)\n",
    "# MODEL_SAVE_DIR = \"./qwen3_8b_with_expanded_tokenizer\"  # 예시\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "988def98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base len: 151669\n",
      "exp  len: 163241\n",
      "delta: 11572\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "base_tok = AutoTokenizer.from_pretrained(BASE_TOK_DIR, use_fast=True)\n",
    "exp_tok  = AutoTokenizer.from_pretrained(EXP_TOK_DIR,  use_fast=True)\n",
    "\n",
    "print(\"base len:\", len(base_tok))\n",
    "print(\"exp  len:\", len(exp_tok))\n",
    "print(\"delta:\", len(exp_tok) - len(base_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee99de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 500\n",
      "base avg: 529.586\n",
      "exp  avg: 505.002\n",
      "delta (base-exp): 24.584000000000003\n",
      "delta %: 4.642116672268528\n"
     ]
    }
   ],
   "source": [
    "import random, numpy as np, datasets\n",
    "\n",
    "data2 = datasets.load_dataset(\"Goedel-LM/Lean-workbook-proofs\", split=\"train\")\n",
    "\n",
    "samples = []\n",
    "for _ in range(500):\n",
    "    t = data2[random.randrange(len(data2))][\"full_proof\"]\n",
    "    if isinstance(t, str) and t.strip():\n",
    "        samples.append(t)\n",
    "\n",
    "base_l = [len(base_tok(s, add_special_tokens=False)[\"input_ids\"]) for s in samples]\n",
    "exp_l  = [len(exp_tok(s,  add_special_tokens=False)[\"input_ids\"]) for s in samples]\n",
    "\n",
    "print(\"N:\", len(samples))\n",
    "print(\"base avg:\", float(np.mean(base_l)))\n",
    "print(\"exp  avg:\", float(np.mean(exp_l)))\n",
    "print(\"delta (base-exp):\", float(np.mean(base_l) - np.mean(exp_l)))\n",
    "print(\"delta %:\", float(100*(np.mean(base_l) - np.mean(exp_l))/np.mean(base_l)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd62ad21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== delta: 154 ===\n",
      "base len: 1753 exp len: 1599\n",
      "base tokens (first 80): ['import', 'ĠMath', 'lib', 'Ċ', 'import', 'ĠA', 'es', 'op', 'ĊĊ', 'set', '_option', 'Ġmax', 'Heart', 'be', 'ats', 'Ġ', '0', 'ĊĊ', 'open', 'ĠBig', 'Operators', 'ĠReal', 'ĠNat', 'ĠTop', 'ology', 'ĠRat', 'ĊĊ', '/-', 'ĠLet', 'Ġ$', 'a', ',b', ',c', 'Ġ\\\\', 'in', 'Ġ[', '0', ',', '1', '].', 'Ġ$', 'ĠPro', 've', 'Ġthat', 'Ġ\\\\', 'n', 'Ġ$$', '\\\\', 'frac', '{', 'a', '+b', '}{', 'b', '+c', '+', '1', '}', '+\\\\', 'frac', '{', 'b', '+c', '}{', 'c', '+a', '+', '2', '}', '+\\\\', 'frac', '{', 'c', '+a', '}{', 'a', '+b', '+', '1', '}\\\\']\n",
      "exp  tokens (first 80): ['import', 'ĠMathlib', 'Ċ', 'import', 'ĠAesop', 'ĊĊ', 'set', '_option', 'Ġmax', 'Heart', 'be', 'ats', 'Ġ', '0', 'ĊĊ', 'open', 'ĠBigOperators', 'ĠReal', 'ĠNat', 'ĠTopology', 'ĠRat', 'ĊĊ', '/-', 'ĠLet', 'Ġ$', 'a', ',b', ',c', 'Ġ\\\\', 'in', 'Ġ[', '0', ',', '1', '].', 'Ġ$', 'ĠPro', 've', 'Ġthat', 'Ġ\\\\', 'n', 'Ġ$$', '\\\\', 'frac', '{a', '+b', '}{', 'b', '+c', '+', '1', '}', '+\\\\', 'frac', '{b', '+c', '}{', 'c', '+a', '+', '2', '}', '+\\\\', 'frac', '{c', '+a', '}{', 'a', '+b', '+', '1', '}\\\\', 'leq', 'Ġ\\\\', 'frac', '{', '1', '1', '}{', '6']\n",
      "\n",
      "=== delta: 122 ===\n",
      "base len: 1814 exp len: 1692\n",
      "base tokens (first 80): ['import', 'ĠMath', 'lib', 'Ċ', 'import', 'ĠA', 'es', 'op', 'ĊĊ', 'set', '_option', 'Ġmax', 'Heart', 'be', 'ats', 'Ġ', '0', 'ĊĊ', 'open', 'ĠBig', 'Operators', 'ĠReal', 'ĠNat', 'ĠTop', 'ology', 'ĠRat', 'ĊĊ', '/-', 'ĠShow', 'Ġthat', 'Ġthe', 'Ġinequality', 'Ġ$', 't', '^', '2', '(xy', '+', 'yz', '+', 'zx', ')+', '2', 't', '(x', '+y', '+z', ')+', '3', 'Ġ\\\\', 'ge', 'q', 'Ġ', '0', '$', 'Ġis', 'Ġequivalent', 'Ġto', 'Ġ$(', 'tx', '+', '1', ')(', 'ty', '+', '1', ')+(', 'ty', '+', '1', ')(', 'tz', '+', '1', ')+(', 'tz', '+', '1', ')(', 'tx']\n",
      "exp  tokens (first 80): ['import', 'ĠMathlib', 'Ċ', 'import', 'ĠAesop', 'ĊĊ', 'set', '_option', 'Ġmax', 'Heart', 'be', 'ats', 'Ġ', '0', 'ĊĊ', 'open', 'ĠBigOperators', 'ĠReal', 'ĠNat', 'ĠTopology', 'ĠRat', 'ĊĊ', '/-', 'ĠShow', 'Ġthat', 'Ġthe', 'Ġinequality', 'Ġ$', 't', '^', '2', '(xy', '+', 'yz', '+', 'zx', ')+', '2', 't', '(x', '+y', '+z', ')+', '3', 'Ġ\\\\', 'geq', 'Ġ', '0', '$', 'Ġis', 'Ġequivalent', 'Ġto', 'Ġ$(', 'tx', '+', '1', ')(', 'ty', '+', '1', ')+(', 'ty', '+', '1', ')(', 'tz', '+', '1', ')+(', 'tz', '+', '1', ')(', 'tx', '+', '1', ')', 'Ġ\\\\', 'geq', 'Ġ']\n",
      "\n",
      "=== delta: 116 ===\n",
      "base len: 2348 exp len: 2232\n",
      "base tokens (first 80): ['import', 'ĠMath', 'lib', 'Ċ', 'import', 'ĠA', 'es', 'op', 'ĊĊ', 'set', '_option', 'Ġmax', 'Heart', 'be', 'ats', 'Ġ', '0', 'ĊĊ', 'open', 'ĠBig', 'Operators', 'ĠReal', 'ĠNat', 'ĠTop', 'ology', 'ĠRat', 'ĊĊ', '/-', 'ĠLet', 'Ġ$', 'P', '(x', ',y', ')$', 'Ġbe', 'Ġthe', 'Ġassertion', 'Ġ$', 'f', '(x', ')f', '(y', ')=', 'f', '\\\\', 'left', '(\\\\', 'frac', '{x', '+y', '}', '2', '\\\\', 'right', ')^', '2', '-f', '\\\\', 'left', '(\\\\', 'frac', '{x', '-y', '}', '2', '\\\\', 'right', ')^', '2', '$', 'Ġ\\\\', 'n', '\\\\n', 'Ġ$', 'P', '(', '0', ',', '0', ')$']\n",
      "exp  tokens (first 80): ['import', 'ĠMathlib', 'Ċ', 'import', 'ĠAesop', 'ĊĊ', 'set', '_option', 'Ġmax', 'Heart', 'be', 'ats', 'Ġ', '0', 'ĊĊ', 'open', 'ĠBigOperators', 'ĠReal', 'ĠNat', 'ĠTopology', 'ĠRat', 'ĊĊ', '/-', 'ĠLet', 'Ġ$', 'P', '(x', ',y', ')$', 'Ġbe', 'Ġthe', 'Ġassertion', 'Ġ$', 'f', '(x', ')f', '(y', ')=', 'f', '\\\\left', '(\\\\', 'frac', '{x', '+y', '}', '2', '\\\\right', ')^', '2', '-f', '\\\\left', '(\\\\', 'frac', '{x', '-y', '}', '2', '\\\\right', ')^', '2', '$', 'Ġ\\\\', 'n', '\\\\n', 'Ġ$', 'P', '(', '0', ',', '0', ')$', 'Ġ$\\\\', 'impl', 'ies', '$', 'Ġ$', 'f', '(', '0', ')=']\n",
      "\n",
      "=== delta: 91 ===\n",
      "base len: 1020 exp len: 929\n",
      "base tokens (first 80): ['import', 'ĠMath', 'lib', 'Ċ', 'import', 'ĠA', 'es', 'op', 'ĊĊ', 'set', '_option', 'Ġmax', 'Heart', 'be', 'ats', 'Ġ', '0', 'ĊĊ', 'open', 'ĠBig', 'Operators', 'ĠReal', 'ĠNat', 'ĠTop', 'ology', 'ĠRat', 'ĊĊ', '/-', 'ĠGiven', 'Ġmatrices', 'Ġ$', 'ĠA', ':', 'Ġ=', '\\\\', 'left', '(\\\\', 'begin', '{', 'matrix', '}', '1', '&', '1', '\\\\\\\\', '0', '&', '0', '\\\\', 'end', '{', 'matrix', '}\\\\', 'right', ')$', 'Ġand', 'Ġ$', 'ĠB', ':', 'Ġ=', '\\\\', 'left', '(\\\\', 'begin', '{', 'matrix', '}', '0', '&', '0', '\\\\\\\\', '1', '&', '1', '\\\\', 'end', '{', 'matrix', '}\\\\', 'right']\n",
      "exp  tokens (first 80): ['import', 'ĠMathlib', 'Ċ', 'import', 'ĠAesop', 'ĊĊ', 'set', '_option', 'Ġmax', 'Heart', 'be', 'ats', 'Ġ', '0', 'ĊĊ', 'open', 'ĠBigOperators', 'ĠReal', 'ĠNat', 'ĠTopology', 'ĠRat', 'ĊĊ', '/-', 'ĠGiven', 'Ġmatrices', 'Ġ$', 'ĠA', ':', 'Ġ=', '\\\\', 'left', '(\\\\', 'begin', '{', 'matrix', '}', '1', '&', '1', '\\\\\\\\', '0', '&', '0', '\\\\end', '{', 'matrix', '}\\\\', 'right', ')$', 'Ġand', 'Ġ$', 'ĠB', ':', 'Ġ=', '\\\\', 'left', '(\\\\', 'begin', '{', 'matrix', '}', '0', '&', '0', '\\\\\\\\', '1', '&', '1', '\\\\end', '{', 'matrix', '}\\\\', 'right', ')$', ',', 'Ġshow', 'Ġthat', 'Ġthey', 'Ġsatisfy', 'Ġ$']\n",
      "\n",
      "=== delta: 90 ===\n",
      "base len: 1030 exp len: 940\n",
      "base tokens (first 80): ['import', 'ĠMath', 'lib', 'Ċ', 'import', 'ĠA', 'es', 'op', 'ĊĊ', 'set', '_option', 'Ġmax', 'Heart', 'be', 'ats', 'Ġ', '0', 'ĊĊ', 'open', 'ĠBig', 'Operators', 'ĠReal', 'ĠNat', 'ĠTop', 'ology', 'ĠRat', 'ĊĊ', '/-', 'ĠLet', 'Ġ$', 'a', ',b', ',c', '$', 'Ġbe', 'Ġnon', '-negative', 'Ġand', 'Ġ$', 'a', '+b', '+c', '=', '1', '.$', 'ĠPro', 've', 'Ġthat', ':', 'Ġ$', '9', '.', 'Ġ', '1', '\\\\', 'le', 'q', '\\\\', 'sqrt', '{', 'a', '^', '2', '+', '4', 'bc', '}', '+\\\\', 'sqrt', '{', 'b', '^', '2', '+', '4', 'ca', '}', '+\\\\', 'sqrt', '{']\n",
      "exp  tokens (first 80): ['import', 'ĠMathlib', 'Ċ', 'import', 'ĠAesop', 'ĊĊ', 'set', '_option', 'Ġmax', 'Heart', 'be', 'ats', 'Ġ', '0', 'ĊĊ', 'open', 'ĠBigOperators', 'ĠReal', 'ĠNat', 'ĠTopology', 'ĠRat', 'ĊĊ', '/-', 'ĠLet', 'Ġ$', 'a', ',b', ',c', '$', 'Ġbe', 'Ġnon', '-negative', 'Ġand', 'Ġ$', 'a', '+b', '+c', '=', '1', '.$', 'ĠPro', 've', 'Ġthat', ':', 'Ġ$', '9', '.', 'Ġ', '1', '\\\\leq', '\\\\sqrt', '{a', '^', '2', '+', '4', 'bc', '}', '+\\\\', 'sqrt', '{b', '^', '2', '+', '4', 'ca', '}', '+\\\\', 'sqrt', '{c', '^', '2', '+', '4', 'ab', '}\\\\', 'leq', '\\\\sqrt', '{', '5']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "diff = np.array(base_l) - np.array(exp_l)\n",
    "best_idx = diff.argsort()[::-1][:5]  # 가장 많이 줄어든 상위 5개\n",
    "\n",
    "for i in best_idx:\n",
    "    s = samples[i]\n",
    "    print(\"\\n=== delta:\", int(diff[i]), \"===\")\n",
    "    print(\"base len:\", base_l[i], \"exp len:\", exp_l[i])\n",
    "    print(\"base tokens (first 80):\", base_tok.tokenize(s)[:80])\n",
    "    print(\"exp  tokens (first 80):\", exp_tok.tokenize(s)[:80])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb091bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg new-token usage ratio: 0.02862622928918279\n",
      "p95 new-token usage ratio: 0.05537101586681906\n"
     ]
    }
   ],
   "source": [
    "old_vocab_size = len(base_tok)\n",
    "\n",
    "def new_token_usage_ratio(text):\n",
    "    ids = exp_tok(text, add_special_tokens=False)[\"input_ids\"]\n",
    "    return sum(i >= old_vocab_size for i in ids) / max(1, len(ids))\n",
    "\n",
    "ratios = [new_token_usage_ratio(s) for s in samples]\n",
    "print(\"avg new-token usage ratio:\", float(np.mean(ratios)))\n",
    "print(\"p95 new-token usage ratio:\", float(np.percentile(ratios,95)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fff5e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top new tokens:\n",
      "2652 _nonneg\n",
      "578 Ġnlinarith\n",
      "561 geq\n",
      "446 Ġ\\]Ċ\n",
      "303 leq\n",
      "221 Ġalgebraic\n",
      "165 {b\n",
      "130 {c\n",
      "123 linarith\n",
      "81 Ġmanipulations\n",
      "76 Ġdenominators\n",
      "73 dfrac\n",
      "69 binom\n",
      "61 \\left\n",
      "55 \\geq\n",
      "54 \\right\n",
      "52 habc\n",
      "46 {R\n",
      "46 Ġyz\n",
      "43 Ġdeduce\n",
      "39 {y\n",
      "38 Ġhz\n",
      "37 âŁ©Ċ\n",
      "37 ^x\n",
      "37 Ġbinomial\n",
      "36 \\sqrt\n",
      "36 {pmatrix\n",
      "34 _emod\n",
      "34 \\ge\n",
      "33 ĠâĪļ(\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "old_vocab_size = len(base_tok)\n",
    "counter = Counter()\n",
    "\n",
    "for s in samples:\n",
    "    ids = exp_tok(s, add_special_tokens=False)[\"input_ids\"]\n",
    "    for i in ids:\n",
    "        if i >= old_vocab_size:\n",
    "            counter[i] += 1\n",
    "\n",
    "top = counter.most_common(30)\n",
    "print(\"Top new tokens:\")\n",
    "for tid, c in top:\n",
    "    print(c, exp_tok.convert_ids_to_tokens(tid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d28f0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c923b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bfe1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e4766dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 200\n",
      "base avg len: 519.905\n",
      "exp  avg len: 496.66\n",
      "avg reduction: 23.244999999999948\n",
      "reduction %: 4.4710091266673615\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import datasets\n",
    "\n",
    "data1 = datasets.load_dataset(\"Goedel-LM/Goedel-Pset-v1\", split=\"train\")\n",
    "data2 = datasets.load_dataset(\"Goedel-LM/Lean-workbook-proofs\", split=\"train\")\n",
    "\n",
    "samples = []\n",
    "for _ in range(200):\n",
    "    ex = data2[random.randrange(len(data2))]\n",
    "    t = ex[\"full_proof\"]\n",
    "    if isinstance(t, str) and t.strip():\n",
    "        samples.append(t)\n",
    "\n",
    "base_lens = []\n",
    "exp_lens  = []\n",
    "\n",
    "for t in samples:\n",
    "    base_lens.append(len(base_tok(t, add_special_tokens=False)[\"input_ids\"]))\n",
    "    exp_lens.append(len(exp_tok(t,  add_special_tokens=False)[\"input_ids\"]))\n",
    "\n",
    "print(\"N =\", len(samples))\n",
    "print(\"base avg len:\", float(np.mean(base_lens)))\n",
    "print(\"exp  avg len:\", float(np.mean(exp_lens)))\n",
    "print(\"avg reduction:\", float(np.mean(base_lens) - np.mean(exp_lens)))\n",
    "print(\"reduction %:\", float(100*(np.mean(base_lens) - np.mean(exp_lens))/np.mean(base_lens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ec39b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
