_wandb:
    value:
        cli_version: 0.21.0
        e:
            5cxt9lan1rkfxn4iy5lfvxc1ig9j7piu:
                args:
                    - --deepspeed
                    - //data/SIML/sy/group_theory/Continual_learning/config/zero3_bf16.json
                    - --output_dir
                    - ./FFT-naive
                    - --model_name_or_path
                    - Qwen/Qwen3-8B
                    - --max_seq_length
                    - "4096"
                    - --learning_rate
                    - "1e-5"
                    - --gradient_accumulation_steps
                    - "8"
                    - --per_device_train_batch_size
                    - "2"
                    - --num_train_epochs
                    - "1"
                    - --wandb_project
                    - continual-learning
                    - --wandb_name
                    - naive-fft
                    - --wandb_tags
                    - fft,naive
                codePath: src/train_nl2fl.py
                codePathLocal: src/train_nl2fl.py
                cpu_count: 124
                cpu_count_logical: 124
                cudaVersion: "12.2"
                disk:
                    /:
                        total: "207929917440"
                        used: "130942189568"
                email: luj0407@gmail.com
                executable: /data/conda_envs/cpt/bin/python3.12
                gpu: NVIDIA H100 80GB HBM3
                gpu_count: 8
                gpu_nvidia:
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "85520809984"
                      name: NVIDIA H100 80GB HBM3
                      uuid: GPU-99f79f2e-99b9-16d5-5b46-46120fa20259
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "85520809984"
                      name: NVIDIA H100 80GB HBM3
                      uuid: GPU-df4b0f24-ddf7-0e94-7226-8cbcdb022cfc
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "85520809984"
                      name: NVIDIA H100 80GB HBM3
                      uuid: GPU-79c58460-90a7-9270-6768-6c2ea8a394ee
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "85520809984"
                      name: NVIDIA H100 80GB HBM3
                      uuid: GPU-6e0426b4-8934-895a-a622-c8dfe1c566b9
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "85520809984"
                      name: NVIDIA H100 80GB HBM3
                      uuid: GPU-9622b9fb-d88b-ff23-e994-23dbcaf10d91
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "85520809984"
                      name: NVIDIA H100 80GB HBM3
                      uuid: GPU-3ed60614-03a1-f14d-cbbb-23569ed2c9d8
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "85520809984"
                      name: NVIDIA H100 80GB HBM3
                      uuid: GPU-889b3b5b-5c93-de20-1261-628598d46b7f
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "85520809984"
                      name: NVIDIA H100 80GB HBM3
                      uuid: GPU-2ec6491d-9eee-09bf-79f3-db3e1e1e026f
                host: gpu-1
                memory:
                    total: "1901975117824"
                os: Linux-5.15.0-117-generic-x86_64-with-glibc2.35
                program: /data/SIML/sy/group_theory/Continual_learning/src/train_nl2fl.py
                python: CPython 3.12.0
                root: /data/SIML/sy/group_theory/Continual_learning
                startedAt: "2025-12-23T07:13:09.141038Z"
                writerId: 5cxt9lan1rkfxn4iy5lfvxc1ig9j7piu
        m: []
        python_version: 3.12.0
        t:
            "1":
                - 1
                - 5
                - 11
                - 41
                - 49
                - 51
                - 53
                - 71
                - 98
                - 105
            "2":
                - 1
                - 5
                - 11
                - 41
                - 49
                - 51
                - 53
                - 71
                - 98
                - 105
            "3":
                - 13
                - 15
                - 16
            "4": 3.12.0
            "5": 0.21.0
            "6": 4.53.1
            "12": 0.21.0
            "13": linux-x86_64
attn_implementation:
    value: null
bf16:
    value: true
dataset_name:
    value: Goedel-LM/Goedel-Pset-v1
deepspeed:
    value: //data/SIML/sy/group_theory/Continual_learning/config/zero3_bf16.json
enable_thinking:
    value: false
eval_ratio:
    value: 0.001
fp16:
    value: false
gradient_accumulation_steps:
    value: 8
gradient_checkpointing:
    value: true
learning_rate:
    value: 1e-05
lr_scheduler_type:
    value: cosine
max_grad_norm:
    value: 1
max_seq_length:
    value: 4096
model_name_or_path:
    value: Qwen/Qwen3-8B
num_train_epochs:
    value: 1
optim:
    value: adamw_torch_fused
output_dir:
    value: ./FFT-naive
per_device_train_batch_size:
    value: 2
tf32:
    value: true
wandb_group:
    value: null
wandb_mode:
    value: online
wandb_project:
    value: continual-learning
warmup_ratio:
    value: 0.03
weight_decay:
    value: 0.1
wrap_in_code_block:
    value: true
